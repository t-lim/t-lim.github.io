{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "513a3250",
   "metadata": {},
   "source": [
    "# Fine-tune LLM for Medical NLI task\n",
    "\n",
    "This tutorial demonstrates how to fine-tune open-source Large Language Models, such as Google's 2024 gemma 2B- and 7B-parameter and Meta's 2023 Llama-2 7B- and 13B-parameter models, for medical NLP tasks such as NLI (natural language inference).  To load, train and run inference on consumer-grade hardware*, \"quantization\" and \"low rank adaptation\" methods are explained and used. The final results using the MedNLI benchmark dataset suggest that gemma-7b-it performed best, slightly outperforming Llama-2-13b.\n",
    "\n",
    "*this notebook was run on an over-2-year-old laptop with a RTX-3080 16GB GPU\n",
    "\n",
    "References:\n",
    "- https://github.com/jgc128/mednli\n",
    "- https://pytorch.org/blog/finetune-llms/\n",
    "- https://github.com/TimDettmers/bitsandbytes\n",
    "- https://github.com/huggingface/peft\n",
    "- https://www.kaggle.com/code/lucamassaron/fine-tune-gemma-7b-it-for-sentiment-analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e50489eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytorch version: 2.2.1+cu121, device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import (AutoModelForCausalLM,\n",
    "                          AutoTokenizer,\n",
    "                          TrainingArguments,\n",
    "                          BitsAndBytesConfig)\n",
    "from peft import LoraConfig, AutoPeftModelForCausalLM\n",
    "from trl import SFTTrainer\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from tqdm import tqdm\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"pytorch version: {torch.__version__}, device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ffdd1e",
   "metadata": {},
   "source": [
    "## Load model from HF hub or checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dff86418",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-trained LLM to use\n",
    "model_id = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "model_id = \"meta-llama/Llama-2-13b-chat-hf\"\n",
    "model_id = \"google/gemma-2b-it\"\n",
    "model_id = \"google/gemma-7b-it\"\n",
    "output_dir = os.path.join('models', model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "add8c95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select to load model from HF hub, or previously checkpoint-saved folder\n",
    "from_checkpoint = model_id\n",
    "#from_checkpoint = output_dir\n",
    "#from_checkpoint = os.path.join(output_dir, 'checkpoint-474')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c206fd",
   "metadata": {},
   "source": [
    "## Set training arguments\n",
    "\n",
    "https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments\n",
    "\n",
    "- optim — The optimizer to use: adamw_hf, adamw_torch, adamw_torch_fused, adamw_apex_fused, adamw_anyprecision or adafactor.\n",
    "\n",
    "- gradient_accumulation_steps — Number of updates steps to accumulate the gradients for, before performing a backward/update pass.\n",
    "\n",
    "- lr_scheduler_type — The scheduler type to use.\n",
    "\n",
    "- learning_rate — The initial learning rate.\n",
    "\n",
    "- weight_decay — The weight decay to apply (if not zero) to all layers except all bias and LayerNorm weights.\n",
    "\n",
    "- max_grad_norm — Maximum gradient norm (for gradient clipping).\n",
    "\n",
    "- fp16 — Whether to use fp16 16-bit (mixed) precision training instead of 32-bit training.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f2adeee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# whether to train and/or evaluate\n",
    "do_train = True\n",
    "do_eval = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53551ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define arguments for trainer\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.001,\n",
    "    max_grad_norm=0.3,\n",
    "    warmup_ratio=0.03,\n",
    "    gradient_accumulation_steps=8,\n",
    "    fp16=True,   # use mixed precision floats\n",
    "\n",
    "    num_train_epochs=3.0,  #1.0,\n",
    "    per_device_train_batch_size=2,\n",
    "    evaluation_strategy='steps',\n",
    "    save_steps=0.2,                  # checkpoint interval, -1 for none\n",
    "    logging_steps=0.2,               # logging interval, -1 for none\n",
    "    eval_steps=0.2,                  # evaluation interval, -1 for non\n",
    "    eval_accumulation_steps=1,       # 1 for less memory but slower\n",
    "    prediction_loss_only=True,       # False for full evaluation\n",
    "    gradient_checkpointing=True,\n",
    "    report_to=\"tensorboard\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c64efa",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "source": [
    "## Get mednli dataset\n",
    "\n",
    "MedNLI: A dataset annotated by doctors, performing a natural language inference task (NLI). The source of premise sentences, was the MIMIC-III v1.3 (Johnson et al., 2016) database, which 2,078,705 clinical notes written by healthcare professionals in English.  The hypothesis sentences were generated by clinicians. They were asked to write three sentences (hypotheses): 1) A clearly true statement, 2) A clearly false statement, and 3) A statement that might be true or false. This procedure produces three training pairs of sentences for each initial premise with three different labels: entailment, contradiction, and neutral.\n",
    "\n",
    "Romanov, Alexey and Shivade, Chaitanya, Lessons from Natural Language Inference in the Clinical Domain, 2018. http://arxiv.org/abs/1808.06752,\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d321004a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper to read jsonl\n",
    "import json\n",
    "def read_jsonl(filename, max_samples=None):\n",
    "    \"\"\"helper to read jsonl files as pandas dataframe\"\"\"\n",
    "    lines = []\n",
    "    with open(filename) as f:\n",
    "        lines = f.read().splitlines()\n",
    "    line_dicts = [json.loads(line) for line in lines]\n",
    "    max_samples = max_samples or len(line_dicts)\n",
    "    return pd.DataFrame(line_dicts).iloc[:max_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11ea31d1",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# read in train, dev and test sets\n",
    "X_train = read_jsonl('mednli/mli_train_v1.jsonl')\n",
    "y_train = X_train['gold_label']\n",
    "X_dev = read_jsonl('mednli/mli_dev_v1.jsonl')\n",
    "y_dev = X_dev['gold_label']\n",
    "X_test = read_jsonl('mednli/mli_test_v1.jsonl')\n",
    "y_test = X_test['gold_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07dd9b20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>gold_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>No history of blood clots or DVTs, has never h...</td>\n",
       "      <td>Patient has angina</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>No history of blood clots or DVTs, has never h...</td>\n",
       "      <td>Patient has had multiple PEs</td>\n",
       "      <td>contradiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>No history of blood clots or DVTs, has never h...</td>\n",
       "      <td>Patient has CAD</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Over the past week PTA he has been more somnol...</td>\n",
       "      <td>He has been less alert over the past week</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Over the past week PTA he has been more somnol...</td>\n",
       "      <td>Over the past week he has been alert and orie...</td>\n",
       "      <td>contradiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Over the past week PTA he has been more somnol...</td>\n",
       "      <td>He is disorientated and complains of weakness</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           sentence1   \n",
       "0  No history of blood clots or DVTs, has never h...  \\\n",
       "1  No history of blood clots or DVTs, has never h...   \n",
       "2  No history of blood clots or DVTs, has never h...   \n",
       "3  Over the past week PTA he has been more somnol...   \n",
       "4  Over the past week PTA he has been more somnol...   \n",
       "5  Over the past week PTA he has been more somnol...   \n",
       "\n",
       "                                           sentence2     gold_label  \n",
       "0                                 Patient has angina     entailment  \n",
       "1                       Patient has had multiple PEs  contradiction  \n",
       "2                                    Patient has CAD        neutral  \n",
       "3         He has been less alert over the past week      entailment  \n",
       "4   Over the past week he has been alert and orie...  contradiction  \n",
       "5     He is disorientated and complains of weakness         neutral  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show premise, hypothesis and label for dev examples\n",
    "X_dev.iloc[:6][['sentence1', 'sentence2', 'gold_label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3220b3ef",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# format premise and hypothesis as chat prompt\n",
    "def as_test_prompt(ex):\n",
    "    \"\"\"prompt for response to test example\"\"\"\n",
    "    return f\"\"\"\n",
    "            Use the following context to determine if the factuality of the\n",
    "            statement enclosed in square brackets at the end is entailment,\n",
    "            neutral, or contradiction, and return the answer in 1 word as\n",
    "            \"entailment\" or \"neutral\" or \"negative\":\n",
    "\n",
    "            {ex['sentence1']}\n",
    "\n",
    "            [{ex['sentence2']}]\n",
    "\n",
    "            Answer:\n",
    "            \"\"\".strip()\n",
    "\n",
    "# format premise and hypothesis with gold label as training example\n",
    "def as_prompt(ex):\n",
    "    \"\"\"training example\"\"\"\n",
    "    return f\"{as_test_prompt(ex)}  {ex['gold_label']}\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "021ee456",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reformat all examples as prompts\n",
    "X_train = pd.DataFrame(X_train.apply(as_prompt, axis=1), columns=[\"prompt\"])\n",
    "train_data = Dataset.from_pandas(X_train)\n",
    "X_dev = pd.DataFrame(X_dev.apply(as_prompt, axis=1), columns=[\"prompt\"])\n",
    "dev_data = Dataset.from_pandas(X_dev)\n",
    "X_test = pd.DataFrame(X_test.apply(as_test_prompt, axis=1), columns=[\"prompt\"])\n",
    "test_data = Dataset.from_pandas(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aea9ede",
   "metadata": {},
   "source": [
    "## Get quantized pre-trained model\n",
    "\n",
    "https://huggingface.co/docs/bitsandbytes/main/en/index\n",
    "\n",
    "bitsandbytes enables accessible large language models via k-bit quantization for PyTorch. 8-bit quantization enables large language model inference with only half the required memory and without any performance degradation. This method is based on vector-wise quantization to quantize most features to 8-bits and separately treating outliers with 16-bit matrix multiplication. 4-bit quantization enables large language model training with several memory-saving techniques that don’t compromise performance. This method quantizes a model to 4-bits and inserts a small set of trainable low-rank adaptation (LoRA) weights to allow training.\n",
    "\n",
    "- quantization: a technique to reduce the computational and memory costs of running inference by representing the weights and activations with low-precision data types like 8-bit integer (int8) instead of the usual 32-bit floating point (float32). Reducing the number of bits means the resulting model requires less memory storage, consumes less energy (in theory), and operations like matrix multiplication can be performed much faster with integer arithmetic. It also allows to run models on embedded devices, which sometimes only support integer data types.\n",
    "\n",
    "- LLM.int8(): a quantization method that doesn’t degrade performance which makes large model inference more accessible. The key is to extract the outliers from the inputs and weights and multiply them in 16-bit. All other values are multiplied in 8-bit and quantized to Int8 before being dequantized back to 16-bits. The outputs from the 16-bit and 8-bit multiplication are combined to produce the final output.\n",
    "\n",
    "- nf4: a quantization data type where each bin has equal area under a standard normal distribution N(0, 1) that is normalized into the range [-1, 1].\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0827f727",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:12<00:00,  3.06s/it]\n"
     ]
    }
   ],
   "source": [
    "# Load model from HF hub or local folder\n",
    "compute_dtype = getattr(torch, \"float16\")\n",
    "if do_train:   # Load and quantize pre-trained model to start fine-tune\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=False,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=compute_dtype\n",
    "    )\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        from_checkpoint,\n",
    "        device_map=device,  # \"auto\" may be slower because offload to cpu\n",
    "        quantization_config=bnb_config\n",
    "    )\n",
    "else:    # Load previously checkpoint-saved model to continue fine-tune or evaluate\n",
    "    model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "        from_checkpoint,\n",
    "        torch_dtype=compute_dtype,\n",
    "        return_dict=False,\n",
    "        low_cpu_mem_usage=True,\n",
    "        device_map=device,  # \"auto\" may be slower because offload to cpu\n",
    "    )\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Fix missing pad_token if error\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': tokenizer.eos_token})\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "13afbf34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA: 6.01 GB\n"
     ]
    }
   ],
   "source": [
    "print(f\"CUDA: {torch.cuda.memory_allocated()/1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a16c2d",
   "metadata": {},
   "source": [
    "\n",
    "## Helpers for evaluating model predictions \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "597967fb",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def predict(X_test, model, tokenizer):\n",
    "    \"\"\"Generate model predictions on test set\"\"\"\n",
    "    y_pred = []\n",
    "    for i in tqdm(range(len(X_test))):\n",
    "        prompt = X_test.iloc[i][\"prompt\"]\n",
    "        input_ids = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "        outputs = model.generate(**input_ids,\n",
    "                                 max_new_tokens=4,  # 8\n",
    "                                 do_sample=False,   # True\n",
    "                                 temperature=0.00,  # 0.01\n",
    "        )\n",
    "        result = tokenizer.decode(outputs[0][len(input_ids[0]):])   \n",
    "        answer = result.lower() # result.split(\"=\")[-1].lower()\n",
    "        if \"entailment\" in answer:\n",
    "            y_pred.append(\"entailment\")\n",
    "        elif \"contradiction\" in answer:\n",
    "            y_pred.append(\"contradiction\")\n",
    "        else:\n",
    "            y_pred.append(\"neutral\")\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "06c29a67",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def evaluate(y_true, y_pred, verbose=False):\n",
    "    \"\"\"Evaluate accuracy and confusion matrix of model predictions\"\"\"\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_true=y_true, y_pred=y_pred)\n",
    "    if verbose:\n",
    "        print(f'Accuracy: {accuracy:.3f}')\n",
    "                \n",
    "    # Generate classification report\n",
    "    class_report = classification_report(y_true=y_true, y_pred=y_pred)\n",
    "    if verbose:\n",
    "        print('\\nClassification Report:')\n",
    "        print(class_report)\n",
    "    \n",
    "    # Generate confusion matrix\n",
    "    conf_matrix = confusion_matrix(y_true=y_true, y_pred=y_pred)\n",
    "    if verbose:\n",
    "        print('\\nConfusion Matrix:')\n",
    "        print(conf_matrix)\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c5b549fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1422/1422 [06:05<00:00,  3.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.333\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "contradiction       0.00      0.00      0.00       474\n",
      "   entailment       0.00      0.00      0.00       474\n",
      "      neutral       0.33      1.00      0.50       474\n",
      "\n",
      "     accuracy                           0.33      1422\n",
      "    macro avg       0.11      0.33      0.17      1422\n",
      " weighted avg       0.11      0.33      0.17      1422\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[  0   0 474]\n",
      " [  0   0 474]\n",
      " [  0   0 474]]\n",
      "Prompt: Use the following context to determine if the factuality of the\n",
      "            statement enclosed in square brackets at the end is entailment,\n",
      "            neutral, or contradiction, and return the answer in 1 word as\n",
      "            \"entailment\" or \"neutral\" or \"negative\":\n",
      "\n",
      "            He could think of what he wanted to say but was having trouble getting the words out.\n",
      "\n",
      "            [ The patient is having trouble speaking. ]\n",
      "\n",
      "            Answer:\n",
      "Answer: \n",
      "\n",
      "the factu\n"
     ]
    }
   ],
   "source": [
    "# Evaluate pre-trained that has *NOT* been tuned for downstream NLI task\n",
    "y_pred = predict(X_test, model, tokenizer)\n",
    "evaluate(y_test, y_pred, verbose=True)\n",
    "\n",
    "prompt = X_test.iloc[42][\"prompt\"]\n",
    "print('Prompt:', prompt)\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(**input_ids,\n",
    "                         max_new_tokens=4,\n",
    "                         do_sample=False,\n",
    "                         temperature=0.00\n",
    "                         )\n",
    "result = tokenizer.decode(outputs[0][len(input_ids[0]):])\n",
    "answer = result.lower()\n",
    "print('Answer:', answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732476af",
   "metadata": {},
   "source": [
    "\n",
    "## Train model\n",
    "\n",
    "- PEFT (Parameter-Efficient Fine-Tuning) methods enable efficient adaptation of large pretrained models to various downstream applications by only fine-tuning a small number of (extra) model parameters instead of all the model's parameters. PEFT can save storage by avoiding full finetuning of models on each of downstream task or dataset. One of the main benefits of using PEFT is the huge savings in compute and storage.\n",
    "\n",
    "- LoRA (Low-Rank Adaptation) works by attaching extra trainable parameters into a model and decomposing a large weight matrix into two smaller, low-rank matrices (called update matrices). These new matrices can be trained to adapt to the new data while keeping the overall number of changes low. The original weight matrix remains frozen and doesn’t receive any further adjustments. To produce the final results, both the original and the adapted weights are combined.\n",
    "\n",
    "- QLoRA is a 4-bit quantization method that enables large language model training with several memory-saving techniques that don’t compromise performance. This method quantizes a model to 4-bits and inserts a small set of trainable low-rank adaptation (LoRA) weights to allow training.\n",
    "\n",
    "https://pytorch.org/blog/finetune-llms/\n",
    "https://github.com/huggingface/peft\n",
    "https://huggingface.co/docs/bitsandbytes/main/en/index\n",
    "\n",
    "Prepare a model for training with a PEFT method such as LoRA by wrapping the base model with PEFT configuration\n",
    "\n",
    "- r — attention dimension (the “rank”)\n",
    "\n",
    "- lora_alpha — scaling factor for the weight matrices, a higher alpha assigns more weight to the LoRA activations\n",
    "\n",
    "- lora_dropout — The dropout probability for Lora layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f1ac62be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set config for PEFT\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    r=64,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1d510ca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 11232/11232 [00:00<00:00, 17323.32 examples/s]\n",
      "Map: 100%|██████████| 1395/1395 [00:00<00:00, 23797.54 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Set config for SFT Trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=dev_data,\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"prompt\",\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    packing=False,\n",
    "    max_seq_length=1024,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b7903d46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 422/2106 [45:33<3:00:58,  6.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8024, 'grad_norm': 0.32632938027381897, 'learning_rate': 0.00018521172305285236, 'epoch': 0.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      " 20%|██        | 422/2106 [47:59<3:00:58,  6.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7946441173553467, 'eval_runtime': 145.5527, 'eval_samples_per_second': 9.584, 'eval_steps_per_second': 1.202, 'epoch': 0.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 844/2106 [1:33:14<2:16:52,  6.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6638, 'grad_norm': 0.3769872784614563, 'learning_rate': 0.00013623384610073693, 'epoch': 1.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \n",
      " 40%|████      | 844/2106 [1:35:36<2:16:52,  6.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7909778952598572, 'eval_runtime': 141.786, 'eval_samples_per_second': 9.839, 'eval_steps_per_second': 1.234, 'epoch': 1.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 1266/2106 [2:20:39<1:30:41,  6.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5796, 'grad_norm': 0.4613594710826874, 'learning_rate': 7.25118606258684e-05, 'epoch': 1.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \n",
      " 60%|██████    | 1266/2106 [2:23:01<1:30:41,  6.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8163220882415771, 'eval_runtime': 141.8245, 'eval_samples_per_second': 9.836, 'eval_steps_per_second': 1.234, 'epoch': 1.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 1688/2106 [3:08:42<45:12,  6.49s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4816, 'grad_norm': 0.5449468493461609, 'learning_rate': 1.9975221274455323e-05, 'epoch': 2.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 80%|████████  | 1688/2106 [3:11:07<45:12,  6.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8947425484657288, 'eval_runtime': 145.2768, 'eval_samples_per_second': 9.602, 'eval_steps_per_second': 1.205, 'epoch': 2.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2106/2106 [3:55:53<00:00,  6.72s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 14153.7833, 'train_samples_per_second': 2.381, 'train_steps_per_second': 0.149, 'train_loss': 0.7942605955987914, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "if do_train:\n",
    "    for _ in range(10):    # reclaim memory before training\n",
    "        with torch.no_grad():\n",
    "            torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    training_stats = trainer.train()\n",
    "\n",
    "    # Save trained model\n",
    "    #trainer.model.save_pretrained(output_dir)\n",
    "    trainer.save_model(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b7bfc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.Series(training_stats.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "91c4e13e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA: 7.90 GB\n"
     ]
    }
   ],
   "source": [
    "print(f\"CUDA: {torch.cuda.memory_allocated()/1e9:.2f} GB\")\n",
    "# !nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b50ccc2",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## Generates responses and evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a33e2333",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1422 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "100%|██████████| 1422/1422 [18:15<00:00,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.878\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "contradiction       0.92      0.94      0.93       474\n",
      "   entailment       0.86      0.87      0.86       474\n",
      "      neutral       0.85      0.82      0.84       474\n",
      "\n",
      "     accuracy                           0.88      1422\n",
      "    macro avg       0.88      0.88      0.88      1422\n",
      " weighted avg       0.88      0.88      0.88      1422\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[446   8  20]\n",
      " [ 15 412  47]\n",
      " [ 23  60 391]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if do_eval:\n",
    "    for _ in range(10):    # reclaim memory\n",
    "        with torch.no_grad():\n",
    "            torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    y_pred = predict(X_test, trainer.model, tokenizer)\n",
    "    evaluate(y_test, y_pred, verbose=True)\n",
    "\n",
    "    evaluation = pd.DataFrame({'prompt': X_test[\"prompt\"],\n",
    "                               'y_test': y_test,\n",
    "                               'y_pred': y_pred})\n",
    "    eval_dir = os.path.join(output_dir, 'logs')\n",
    "    os.makedirs(eval_dir, exist_ok=True)\n",
    "    evaluation.to_csv(\n",
    "        os.path.join(eval_dir, datetime.now().strftime(\"%Y%m%d-%H%M\")),\n",
    "        index=False\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
